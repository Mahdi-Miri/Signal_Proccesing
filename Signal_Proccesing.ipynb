{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Mahdi-Miri/Signal_Proccesing-/blob/main/Signal_Proccesing.ipynb",
      "authorship_tag": "ABX9TyMBFSwCCE8UA+kYq3t1v+mU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahdi-Miri/Signal_Proccesing/blob/main/Signal_Proccesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Readig"
      ],
      "metadata": {
        "id": "FTuvlrstUHeC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8fe35a",
        "outputId": "d02e53fe-b6a4-4e59-dcb4-35725f32ffa9"
      },
      "source": [
        "!pip install obspy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: obspy in /usr/local/lib/python3.12/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from obspy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from obspy) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from obspy) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from obspy) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from obspy) (75.2.0)\n",
            "Requirement already satisfied: sqlalchemy<2 in /usr/local/lib/python3.12/dist-packages (from obspy) (1.4.54)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from obspy) (2.32.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (2.9.0.post0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2->obspy) (3.2.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling"
      ],
      "metadata": {
        "id": "3yqHTrHlNELT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D,\n",
        "    GlobalAveragePooling2D, TimeDistributed, Dense, Permute, Reshape, Layer\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xw-UFeFjNI11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Custom Layer Definition with Functional Implementation ---\n",
        "# This layer now has a functional implementation based on the\n",
        "# Auto-Correlation mechanism using Fast Fourier Transform (FFT).\n",
        "class CustomAutocorrelationLayer(Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that calculates the temporal auto-correlation of an input tensor.\n",
        "    It operates on the last axis, which is assumed to be the time dimension.\n",
        "    The calculation is performed efficiently in the frequency domain using FFT.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAutocorrelationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The input shape is expected to be (Batch, Height, Width, TimeSteps)\n",
        "        # We need to compute auto-correlation along the TimeSteps axis (axis=-1).\n",
        "\n",
        "        # Get the length of the time series\n",
        "        sequence_length = tf.shape(inputs)[-1]\n",
        "\n",
        "        # --- Step 1: Go to Frequency Domain using FFT ---\n",
        "        # Perform Fast Fourier Transform. tf.signal.rfft is used for real-valued inputs.\n",
        "        # The length of the FFT is padded to the next power of 2 for efficiency,\n",
        "        # but for simplicity, we'll use the original length here.\n",
        "        fft_result = tf.signal.rfft(inputs, fft_length=[sequence_length])\n",
        "\n",
        "        # --- Step 2: Compute Power Spectral Density ---\n",
        "        # This is where the correlation is calculated.\n",
        "        # It's done by multiplying the FFT result by its complex conjugate.\n",
        "        # This is equivalent to convolution in the time domain.\n",
        "        power_spectral_density = fft_result * tf.math.conj(fft_result)\n",
        "\n",
        "        # --- Step 3: Go back to Time Domain using Inverse FFT ---\n",
        "        # Perform Inverse Real Fast Fourier Transform to get the auto-correlation series.\n",
        "        autocorr_result = tf.signal.irfft(power_spectral_density, fft_length=[sequence_length])\n",
        "\n",
        "        return autocorr_result\n",
        "\n",
        "    def get_config(self):\n",
        "        # Required for model saving and loading\n",
        "        config = super(CustomAutocorrelationLayer, self).get_config()\n",
        "        return config\n",
        "\n",
        "# --- Full Architecture Definition ---\n",
        "def build_spatio_temporal_model(input_shape=(60, 60, 100)):\n",
        "    \"\"\"\n",
        "    Builds the complete Spatio-Temporal feature extraction model based on the diagram.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): The shape of the input data (Height, Width, TimeSteps).\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "    # Define the input layer for our spatio-temporal data\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- Part 1: Temporal Transformation & Reshaping ---\n",
        "\n",
        "    # Apply the functional autocorrelation layer to find temporal patterns.\n",
        "    # This layer processes each spatial point's time series (60x60 series of length 100).\n",
        "    # Output values now represent the strength of temporal correlations.\n",
        "    temporal_features = CustomAutocorrelationLayer()(inputs)  # Shape: (None, 60, 60, 100)\n",
        "\n",
        "    # Permute the dimensions to bring the time-steps to the front for the next stage.\n",
        "    # (Height, Width, TimeSteps) -> (TimeSteps, Height, Width)\n",
        "    permuted = Permute((3, 1, 2))(temporal_features)  # Shape: (None, 100, 60, 60)\n",
        "\n",
        "    # Reshape to add a 'channels' dimension. Each time-step is now a 60x60x1 image,\n",
        "    # ready to be processed by a 2D CNN.\n",
        "    reshaped = Reshape((100, 60, 60, 1))(permuted)  # Shape: (None, 100, 60, 60, 1)\n",
        "\n",
        "    # --- Part 2: Time-Distributed Spatial Feature Extraction ---\n",
        "\n",
        "    # Define the CNN block that extracts spatial features from a single 60x60 frame.\n",
        "    cnn_block = tf.keras.Sequential([\n",
        "        # Finds low-level features like edges and gradients.\n",
        "        Conv2D(32, kernel_size=(3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        ReLU(),\n",
        "        # Downsamples the feature map to make representations more robust.\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        # Finds higher-level features by combining low-level ones.\n",
        "        Conv2D(64, kernel_size=(3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        ReLU(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        # Summarizes all spatial features in the frame into a single, fixed-size vector.\n",
        "        GlobalAveragePooling2D()\n",
        "    ], name='internal_cnn_block')\n",
        "\n",
        "    # Apply the defined cnn_block to each of the 100 time-steps independently.\n",
        "    # The output is a sequence of 100 feature vectors, one for each time-step.\n",
        "    time_distributed_cnn = TimeDistributed(cnn_block)(reshaped)  # Shape: (None, 100, 64)\n",
        "\n",
        "    # --- Part 3: Final Transformation ---\n",
        "\n",
        "    # A Dense layer refines the features from the CNN, reducing each feature vector's size to 10.\n",
        "    # It learns combinations of the spatial features.\n",
        "    dense_features = Dense(10, activation='relu')(time_distributed_cnn)  # Shape: (None, 100, 10)\n",
        "\n",
        "    # Final permutation to get the output shape of (Features, TimeSteps).\n",
        "    # This format might be useful for downstream tasks that analyze each feature over time.\n",
        "    final_output = Permute((2, 1))(dense_features)  # Shape: (None, 10, 100)\n",
        "\n",
        "    # Create the final model by defining its inputs and outputs\n",
        "    model = Model(inputs=inputs, outputs=final_output, name='SpatioTemporal_Feature_Extractor')\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Model Creation, Compilation, and Execution Example ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the input shape as per the diagram\n",
        "    INPUT_SHAPE = (60, 60, 100)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_spatio_temporal_model(input_shape=INPUT_SHAPE)\n",
        "\n",
        "    # Compile the model. A loss function and optimizer are needed for training.\n",
        "    # For feature extraction, you might not train it, but compilation is a good practice.\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Print the model summary to verify the architecture and shapes\n",
        "    print(\"--- Model Summary ---\")\n",
        "    model.summary()\n",
        "\n",
        "    # --- Example of running the model with dummy data ---\n",
        "    print(\"\\n--- Running a test prediction ---\")\n",
        "    # Create a batch of 2 dummy \"videos\" with random data\n",
        "    dummy_data = np.random.rand(2, *INPUT_SHAPE)\n",
        "\n",
        "    # Get the model's prediction\n",
        "    predictions = model.predict(dummy_data)\n",
        "\n",
        "    # Print the shape of the output to confirm it matches the design\n",
        "    print(f\"Input data shape: {dummy_data.shape}\")\n",
        "    print(f\"Final output shape: {predictions.shape}\")\n"
      ],
      "metadata": {
        "id": "o0hhgiZvNFMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sYcebQYynz8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##testing"
      ],
      "metadata": {
        "id": "dmn17kIoqnYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from obspy.clients.fdsn import Client\n",
        "client = Client(\"IRIS\")  # Initialize Client to download data from IRIS\n",
        "import glob\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from obspy.core import UTCDateTime, read, Stream\n",
        "from obspy.core.inventory.inventory import read_inventory"
      ],
      "metadata": {
        "id": "iWEJwDo7RuWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date_maker(year, month, day,hour, minute, second):\n",
        "    year = str(int(year))\n",
        "    month = str(int(month))\n",
        "    day = str(int(day))\n",
        "    hour = str(int(hour))\n",
        "    minute = str(int(minute))\n",
        "    second = str(int(second))\n",
        "\n",
        "    if len(month) == 1:\n",
        "        month = '0' + month\n",
        "    if len(day) == 1:\n",
        "        day = '0' + day\n",
        "    if len(hour) == 1:\n",
        "        hour = '0' + hour\n",
        "    if len(minute) == 1:\n",
        "        minute = '0' + minute\n",
        "    if len(second) == 1:\n",
        "        second = '0' + second\n",
        "\n",
        "    YMDHMS = year + \"-\" + month + \"-\" + day + \"-\" + hour + \"-\" + minute + \"-\" + second\n",
        "    return YMDHMS"
      ],
      "metadata": {
        "id": "89DvFhzaRpnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrum(data, sampling_rate, title):\n",
        "    n = len(data)\n",
        "    fft_data = np.fft.rfft(data)\n",
        "    freqs = np.fft.rfftfreq(n, 1 / sampling_rate)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(freqs, np.abs(fft_data))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6D3IOvLURvhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = \"GH\"\n",
        "# station_list = ['AKOS', 'KLEF', 'KUKU', 'MRON']\n",
        "station_list = ['AKOS']\n",
        "YMDHMS = date_maker(2013, 1, 1, 0, 0, 0.0)\n",
        "Main_direc = \"/content/wave\"\n",
        "st = read(\"/content/drive/MyDrive/Signal/2013-01-01-00-00-00.mseed\")\n",
        "st.plot()\n",
        "start_time = UTCDateTime(2013, 1, 1, 0, 0, 0.0)\n",
        "xml_dir = \"/content/drive/MyDrive/Signal/\"\n",
        "xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n",
        "inventory = None"
      ],
      "metadata": {
        "id": "G_lV7SPXRxdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for xml_file in xml_files:\n",
        "    inv = read_inventory(xml_file)\n",
        "    if inventory is None:\n",
        "        inventory = inv\n",
        "    else:\n",
        "        inventory += inv"
      ],
      "metadata": {
        "id": "LukqqJSAR0-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_filt = (2, 4, 40.0, 50.0)"
      ],
      "metadata": {
        "id": "HkWuBFEYR4Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for station in station_list:\n",
        "    print(station)\n",
        "    for day in range(2):  # Adjust the range as per the actual number of days\n",
        "        print(day)\n",
        "        day_start = start_time + day * 86400  # 86400 seconds in a day\n",
        "        day_end = day_start + 86400\n",
        "        st_day = st.select(station=station).slice(starttime=day_start, endtime=day_end)\n",
        "        # st_day.remove_response(inventory=inventory, pre_filt=pre_filt)\n",
        "        # savemat('/home/shazam/Tokyo/Ghana_Collaboration/Matlab_codes/day_data_remove_response.mat', {'day_data_remove_response': st_day.traces[0].data})\n",
        "        # st_day.detrend(\"spline\", order=3, dspline=500)\n",
        "        sampling_rate = st.traces[0].stats.sampling_rate\n",
        "        # freq = 6.05  # Frequency to remove\n",
        "        # df = 0.5  # Width of the notch (0.5 Hz range for fine removal)\n",
        "        # freqmin = freq - df / 2.0\n",
        "        # freqmax = freq + df / 2.0\n",
        "        # Initialize a stream for stacking the daily PACs\n",
        "        daily_pac_stack = Stream()\n",
        "        for hour in range(24):\n",
        "            print(hour)\n",
        "            hour_start = day_start + hour * 3600  # 3600 seconds in an hour\n",
        "            hour_end = hour_start + 3600\n",
        "            st_hour = st_day.slice(starttime=hour_start, endtime=hour_end)\n",
        "            # st_hour.detrend(\"spline\", order=5, dspline=1000)\n",
        "            try:\n",
        "                st_hour.detrend(\"spline\", order=5, dspline=1000)\n",
        "            except ValueError as e:\n",
        "                if \"Interior knots t must satisfy Schoenberg-Whitney conditions\" in str(e):\n",
        "                    print(f\"Skipping iteration due to spline error: {e}\")\n",
        "                    continue  # Skip this iteration and proceed with the next one\n",
        "                else:\n",
        "                    raise  # If it's a different ValueError, re-raise it\n",
        "            st_hour.filter(\"highpass\", freq=1.3, corners=3, zerophase=True)\n",
        "            # st_hour.filter(\"lowpass\", freq=13, corners=3, zerophase=True)\n",
        "            while True:\n",
        "                n = len(st_hour.traces[0].data)\n",
        "                dt = st_hour.traces[0].stats.delta  # Sampling interval\n",
        "                freqs = np.fft.rfftfreq(n, dt)  # Frequency array\n",
        "                fft_vals = np.fft.rfft(st_hour.traces[0].data)  # Fourier transform\n",
        "                amplitude = np.abs(fft_vals)\n",
        "                max_index = np.argmax(amplitude[0:45000])\n",
        "                notch_freq = freqs[max_index]\n",
        "                quality_factor = 30  # Quality factor (higher value = narrower notch)\n",
        "                sampling_rate = st_hour[0].stats.sampling_rate  # Sampling rate (Hz)\n",
        "                b, a = iirnotch(w0=notch_freq, Q=quality_factor, fs=sampling_rate)\n",
        "                filtered_stream = st_hour.copy()\n",
        "                for trace in st_hour:\n",
        "                    trace.data = filtfilt(b, a, trace.data)\n",
        "                if 1==1:\n",
        "                    break\n",
        "            if len(st_hour) == 0:\n",
        "                print(f\"Data gap detected for {station} on {day_start}. Skipping this hour.\")\n",
        "                continue\n",
        "            try:\n",
        "                st_hour.remove_response(inventory=inventory, pre_filt=pre_filt)\n",
        "            except ValueError as e:\n",
        "                print(f\"ValueError encountered while processing {station} at hour {hour} on {day_start}: {e}\")\n",
        "                continue  # Skip this hour if there's an error\n",
        "            st_hour.detrend(\"spline\", order=3, dspline=700)\n",
        "            output_dir = \"/content/AutoCorr_Outputs\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            # Combine all traces in this hour into a 2D array\n",
        "            try:\n",
        "                hour_data = np.stack([tr.data for tr in st_hour], axis=0)  # shape: (n_traces, n_samples)\n",
        "            except ValueError as e:\n",
        "                print(f\"Skipping {station} day {day} hour {hour} — inconsistent trace lengths: {e}\")\n",
        "                continue\n",
        "            filename = f\"{station}_day{day}_hour{hour}.npy\"\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            # 2. Save the numpy array to the specified path\n",
        "            np.save(output_path, hour_data)\n",
        "            print(f\"Successfully saved data to {output_path}\")"
      ],
      "metadata": {
        "id": "46cwckJTR7sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import (\n",
        "    Input, Permute, Reshape, Conv2D, BatchNormalization,\n",
        "    ReLU, MaxPooling2D, GlobalAveragePooling2D, TimeDistributed, Dense, Layer\n",
        ")\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# 1️⃣ Custom Keras Layer Definition\n",
        "# ============================================================\n",
        "\n",
        "class CustomAutocorrelationLayer(Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that calculates the temporal auto-correlation of an input tensor.\n",
        "    It operates on the last axis, which is assumed to be the time dimension.\n",
        "    The calculation is performed efficiently in the frequency domain using FFT.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAutocorrelationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Input shape: (Batch, Height, Width, TimeSteps)\n",
        "        sequence_length = tf.shape(inputs)[-1]\n",
        "\n",
        "        # Step 1: Go to Frequency Domain using FFT\n",
        "        fft_result = tf.signal.rfft(inputs, fft_length=[sequence_length])\n",
        "\n",
        "        # Step 2: Compute Power Spectral Density (FFT * Conjugate[FFT])\n",
        "        power_spectral_density = fft_result * tf.math.conj(fft_result)\n",
        "\n",
        "        # Step 3: Go back to Time Domain using Inverse FFT\n",
        "        autocorr_result = tf.signal.irfft(power_spectral_density, fft_length=[sequence_length])\n",
        "\n",
        "        return autocorr_result\n",
        "\n",
        "    def get_config(self):\n",
        "        # Required for model saving and loading\n",
        "        config = super(CustomAutocorrelationLayer, self).get_config()\n",
        "        return config\n",
        "\n",
        "# ============================================================\n",
        "# 2️⃣ Spatio-Temporal Model Architecture\n",
        "# ============================================================\n",
        "\n",
        "def build_spatio_temporal_model(input_shape=(60, 60, 100)):\n",
        "    \"\"\"\n",
        "    Builds the complete Spatio-Temporal feature extraction model.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- Part 1: Temporal Transformation & Reshaping ---\n",
        "    temporal_features = CustomAutocorrelationLayer()(inputs)\n",
        "    permuted = Permute((3, 1, 2))(temporal_features)\n",
        "    reshaped = Reshape((100, 60, 60, 1))(permuted)\n",
        "\n",
        "    # --- Part 2: Time-Distributed Spatial Feature Extraction ---\n",
        "    cnn_block = tf.keras.Sequential([\n",
        "        Conv2D(32, kernel_size=(3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        ReLU(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, kernel_size=(3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        ReLU(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        GlobalAveragePooling2D()\n",
        "    ], name='internal_cnn_block')\n",
        "\n",
        "    time_distributed_cnn = TimeDistributed(cnn_block)(reshaped)\n",
        "\n",
        "    # --- Part 3: Final Transformation ---\n",
        "    dense_features = Dense(10, activation='relu')(time_distributed_cnn)\n",
        "    final_output = Permute((2, 1))(dense_features)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=final_output, name='SpatioTemporal_Feature_Extractor')\n",
        "    return model\n",
        "\n",
        "# ============================================================\n",
        "# 3️⃣ Data Loading and Preparation\n",
        "# ============================================================\n",
        "\n",
        "def load_autocorr_data(path=\"AutoCorr_Outputs\", input_shape=(60, 60, 100)):\n",
        "    \"\"\"\n",
        "    Loads all .npy files, reshapes, and normalizes them.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"⚠️ Directory '{path}' not found. Creating dummy data for demonstration.\")\n",
        "        num_samples = 20\n",
        "        X = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
        "        print(f\"✅ Created {num_samples} dummy samples with shape {X.shape}\")\n",
        "        return X\n",
        "\n",
        "    files = sorted(glob.glob(os.path.join(path, \"*.npy\")))\n",
        "    if len(files) == 0:\n",
        "        raise FileNotFoundError(f\"No .npy files found in {path}\")\n",
        "\n",
        "    X = []\n",
        "    for f in files:\n",
        "        data = np.load(f)\n",
        "        data = np.nan_to_num(data)\n",
        "        flat = data.flatten()\n",
        "        total_needed = np.prod(input_shape)\n",
        "\n",
        "        if flat.size < total_needed:\n",
        "            flat = np.pad(flat, (0, total_needed - flat.size))\n",
        "        elif flat.size > total_needed:\n",
        "            flat = flat[:total_needed]\n",
        "\n",
        "        reshaped = flat.reshape(input_shape)\n",
        "        # Normalize each sample independently\n",
        "        mean = np.mean(reshaped)\n",
        "        std = np.std(reshaped)\n",
        "        reshaped = (reshaped - mean) / (std + 1e-8)\n",
        "        X.append(reshaped)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    print(f\"✅ Loaded {len(X)} samples from {path}, shape per sample: {X.shape[1:]}\")\n",
        "    return X\n",
        "\n",
        "# ============================================================\n",
        "# 4️⃣ Model Loading Helper\n",
        "# ============================================================\n",
        "\n",
        "def build_or_load_model(input_shape=(60, 60, 100), model_path=\"trained_spatio_temporal_model.h5\"):\n",
        "    \"\"\"\n",
        "    Loads a previously saved model if it exists, otherwise builds a new one.\n",
        "    \"\"\"\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"📂 Found existing model at {model_path}, loading...\")\n",
        "        model = tf.keras.models.load_model(\n",
        "            model_path,\n",
        "            custom_objects={'CustomAutocorrelationLayer': CustomAutocorrelationLayer}\n",
        "        )\n",
        "    else:\n",
        "        print(\"🧠 Building a new spatio-temporal model...\")\n",
        "        model = build_spatio_temporal_model(input_shape)\n",
        "\n",
        "    # Always compile the model to ensure it's ready for training or evaluation\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    print(\"--- Model Summary ---\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# ============================================================\n",
        "# 5️⃣ Training Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"\n",
        "    Main function to run the data loading, model building, and training process.\n",
        "    \"\"\"\n",
        "    # --- Configuration ---\n",
        "    INPUT_SHAPE = (60, 60, 100)\n",
        "    MODEL_PATH = \"trained_spatio_temporal_model.h5\"\n",
        "    DATA_PATH = \"AutoCorr_Outputs\" # Change this to your data directory\n",
        "\n",
        "    # --- Load Data ---\n",
        "    X = load_autocorr_data(path=DATA_PATH, input_shape=INPUT_SHAPE)\n",
        "\n",
        "    # --- Dummy Labels (replace with real targets) ---\n",
        "    # The model's output shape is (Batch, 10, 100)\n",
        "    y = np.random.rand(len(X), 10, 100).astype(np.float32)\n",
        "    print(f\"🔹 Using dummy labels with shape {y.shape} for training demonstration.\")\n",
        "\n",
        "    # --- Split Data ---\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # --- Build or Load Model ---\n",
        "    model = build_or_load_model(INPUT_SHAPE, MODEL_PATH)\n",
        "\n",
        "    # --- Callbacks ---\n",
        "    checkpoint = ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # --- Train Model ---\n",
        "    print(\"\\n🚀 Starting training...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=20,\n",
        "        batch_size=4,\n",
        "        callbacks=[checkpoint, early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- Final Save and Summary ---\n",
        "    model.save(MODEL_PATH)\n",
        "    print(f\"\\n✅ Training complete. Final model saved to {MODEL_PATH}\")\n",
        "\n",
        "    # --- Plot Training History ---\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(\"Model Training History\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================\n",
        "# 6️⃣ Run the Training Process\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Note: This script expects .npy files in a directory named \"AutoCorr_Outputs\".\n",
        "    # If this directory doesn't exist, it will automatically generate random dummy\n",
        "    # data to demonstrate the training process.\n",
        "    trained_model = train_model()"
      ],
      "metadata": {
        "id": "YcAhfELBfNhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHpGPbUVgQBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}