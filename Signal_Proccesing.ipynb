{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Mahdi-Miri/Signal_Proccesing/blob/main/Signal_Proccesing.ipynb",
      "authorship_tag": "ABX9TyM7rSikH79zm76UjAFqT4H6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahdi-Miri/Signal_Proccesing/blob/main/Signal_Proccesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Readig"
      ],
      "metadata": {
        "id": "FTuvlrstUHeC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8fe35a",
        "outputId": "55d3d04c-a8b6-4e03-effa-a5453da8dbb4"
      },
      "source": [
        "!pip install obspy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting obspy\n",
            "  Downloading obspy-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from obspy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from obspy) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from obspy) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from obspy) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from obspy) (75.2.0)\n",
            "Collecting sqlalchemy<2 (from obspy)\n",
            "  Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from obspy) (2.32.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (2.9.0.post0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2->obspy) (3.2.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.17.0)\n",
            "Downloading obspy-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlalchemy, obspy\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.44\n",
            "    Uninstalling SQLAlchemy-2.0.44:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.44\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "google-adk 1.16.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed obspy-1.4.2 sqlalchemy-1.4.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1"
      ],
      "metadata": {
        "id": "dmn17kIoqnYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from obspy.clients.fdsn import Client\n",
        "client = Client(\"IRIS\")  # Initialize Client to download data from IRIS\n",
        "import glob\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from obspy.core import UTCDateTime, read, Stream\n",
        "from obspy.core.inventory.inventory import read_inventory"
      ],
      "metadata": {
        "id": "iWEJwDo7RuWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date_maker(year, month, day,hour, minute, second):\n",
        "    year = str(int(year))\n",
        "    month = str(int(month))\n",
        "    day = str(int(day))\n",
        "    hour = str(int(hour))\n",
        "    minute = str(int(minute))\n",
        "    second = str(int(second))\n",
        "\n",
        "    if len(month) == 1:\n",
        "        month = '0' + month\n",
        "    if len(day) == 1:\n",
        "        day = '0' + day\n",
        "    if len(hour) == 1:\n",
        "        hour = '0' + hour\n",
        "    if len(minute) == 1:\n",
        "        minute = '0' + minute\n",
        "    if len(second) == 1:\n",
        "        second = '0' + second\n",
        "\n",
        "    YMDHMS = year + \"-\" + month + \"-\" + day + \"-\" + hour + \"-\" + minute + \"-\" + second\n",
        "    return YMDHMS"
      ],
      "metadata": {
        "id": "89DvFhzaRpnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrum(data, sampling_rate, title):\n",
        "    n = len(data)\n",
        "    fft_data = np.fft.rfft(data)\n",
        "    freqs = np.fft.rfftfreq(n, 1 / sampling_rate)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(freqs, np.abs(fft_data))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6D3IOvLURvhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = \"GH\"\n",
        "# station_list = ['AKOS', 'KLEF', 'KUKU', 'MRON']\n",
        "station_list = ['AKOS']\n",
        "YMDHMS = date_maker(2013, 1, 1, 0, 0, 0.0)\n",
        "Main_direc = \"/content/wave\"\n",
        "st = read(\"/content/drive/MyDrive/Signal/2013-01-01-00-00-00.mseed\")\n",
        "st.plot()\n",
        "start_time = UTCDateTime(2013, 1, 1, 0, 0, 0.0)\n",
        "xml_dir = \"/content/drive/MyDrive/Signal/\"\n",
        "xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n",
        "inventory = None"
      ],
      "metadata": {
        "id": "G_lV7SPXRxdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for xml_file in xml_files:\n",
        "    inv = read_inventory(xml_file)\n",
        "    if inventory is None:\n",
        "        inventory = inv\n",
        "    else:\n",
        "        inventory += inv"
      ],
      "metadata": {
        "id": "LukqqJSAR0-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_filt = (2, 4, 40.0, 50.0)"
      ],
      "metadata": {
        "id": "HkWuBFEYR4Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for station in station_list:\n",
        "    print(station)\n",
        "    for day in range(2):  # Adjust the range as per the actual number of days\n",
        "        print(day)\n",
        "        day_start = start_time + day * 86400  # 86400 seconds in a day\n",
        "        day_end = day_start + 86400\n",
        "        st_day = st.select(station=station).slice(starttime=day_start, endtime=day_end)\n",
        "        # st_day.remove_response(inventory=inventory, pre_filt=pre_filt)\n",
        "        # savemat('/home/shazam/Tokyo/Ghana_Collaboration/Matlab_codes/day_data_remove_response.mat', {'day_data_remove_response': st_day.traces[0].data})\n",
        "        # st_day.detrend(\"spline\", order=3, dspline=500)\n",
        "        sampling_rate = st.traces[0].stats.sampling_rate\n",
        "        # freq = 6.05  # Frequency to remove\n",
        "        # df = 0.5  # Width of the notch (0.5 Hz range for fine removal)\n",
        "        # freqmin = freq - df / 2.0\n",
        "        # freqmax = freq + df / 2.0\n",
        "        # Initialize a stream for stacking the daily PACs\n",
        "        daily_pac_stack = Stream()\n",
        "        for hour in range(24):\n",
        "            print(hour)\n",
        "            hour_start = day_start + hour * 3600  # 3600 seconds in an hour\n",
        "            hour_end = hour_start + 3600\n",
        "            st_hour = st_day.slice(starttime=hour_start, endtime=hour_end)\n",
        "            # st_hour.detrend(\"spline\", order=5, dspline=1000)\n",
        "            try:\n",
        "                st_hour.detrend(\"spline\", order=5, dspline=1000)\n",
        "            except ValueError as e:\n",
        "                if \"Interior knots t must satisfy Schoenberg-Whitney conditions\" in str(e):\n",
        "                    print(f\"Skipping iteration due to spline error: {e}\")\n",
        "                    continue  # Skip this iteration and proceed with the next one\n",
        "                else:\n",
        "                    raise  # If it's a different ValueError, re-raise it\n",
        "            st_hour.filter(\"highpass\", freq=1.3, corners=3, zerophase=True)\n",
        "            # st_hour.filter(\"lowpass\", freq=13, corners=3, zerophase=True)\n",
        "            while True:\n",
        "                n = len(st_hour.traces[0].data)\n",
        "                dt = st_hour.traces[0].stats.delta  # Sampling interval\n",
        "                freqs = np.fft.rfftfreq(n, dt)  # Frequency array\n",
        "                fft_vals = np.fft.rfft(st_hour.traces[0].data)  # Fourier transform\n",
        "                amplitude = np.abs(fft_vals)\n",
        "                max_index = np.argmax(amplitude[0:45000])\n",
        "                notch_freq = freqs[max_index]\n",
        "                quality_factor = 30  # Quality factor (higher value = narrower notch)\n",
        "                sampling_rate = st_hour[0].stats.sampling_rate  # Sampling rate (Hz)\n",
        "                b, a = iirnotch(w0=notch_freq, Q=quality_factor, fs=sampling_rate)\n",
        "                filtered_stream = st_hour.copy()\n",
        "                for trace in st_hour:\n",
        "                    trace.data = filtfilt(b, a, trace.data)\n",
        "                if 1==1:\n",
        "                    break\n",
        "            if len(st_hour) == 0:\n",
        "                print(f\"Data gap detected for {station} on {day_start}. Skipping this hour.\")\n",
        "                continue\n",
        "            try:\n",
        "                st_hour.remove_response(inventory=inventory, pre_filt=pre_filt)\n",
        "            except ValueError as e:\n",
        "                print(f\"ValueError encountered while processing {station} at hour {hour} on {day_start}: {e}\")\n",
        "                continue  # Skip this hour if there's an error\n",
        "            st_hour.detrend(\"spline\", order=3, dspline=700)\n",
        "            output_dir = \"/content/AutoCorr_Outputs\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            # Combine all traces in this hour into a 2D array\n",
        "            try:\n",
        "                hour_data = np.stack([tr.data for tr in st_hour], axis=0)  # shape: (n_traces, n_samples)\n",
        "            except ValueError as e:\n",
        "                print(f\"Skipping {station} day {day} hour {hour} — inconsistent trace lengths: {e}\")\n",
        "                continue\n",
        "            filename = f\"{station}_day{day}_hour{hour}.npy\"\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            # 2. Save the numpy array to the specified path\n",
        "            np.save(output_path, hour_data)\n",
        "            print(f\"Successfully saved data to {output_path}\")"
      ],
      "metadata": {
        "id": "46cwckJTR7sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from obspy import Stream\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "# Assuming other necessary imports like obspy's read, UTCDateTime, etc., are already present\n",
        "# from obspy import read, UTCDateTime\n",
        "# from obspy.clients.fdsn import Client\n",
        "\n",
        "# --- Placeholder for variables defined earlier in your script ---\n",
        "# Example placeholders, replace with your actual objects\n",
        "# inventory = Client(\"GFZ\").get_stations(...)\n",
        "# st = read(\"your_data.mseed\")\n",
        "# start_time = UTCDateTime(\"2025-01-01\")\n",
        "# station_list = [\"STN1\", \"STN2\"]\n",
        "# pre_filt = [0.001, 0.005, 45, 50]\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "\n",
        "for station in station_list:\n",
        "    print(station)\n",
        "    for day in range(1):  # Adjust the range as per the actual number of days\n",
        "        print(day)\n",
        "        day_start = start_time + day * 86400  # 86400 seconds in a day\n",
        "        day_end = day_start + 86400\n",
        "        st_day = st.select(station=station).slice(starttime=day_start, endtime=day_end)\n",
        "\n",
        "        sampling_rate = st.traces[0].stats.sampling_rate\n",
        "\n",
        "        # Initialize a stream for stacking the daily PACs\n",
        "        daily_pac_stack = Stream()\n",
        "\n",
        "        for hour in range(24):\n",
        "            print(hour)\n",
        "            hour_start = day_start + hour * 3600  # 3600 seconds in an hour\n",
        "            hour_end = hour_start + 3600\n",
        "            st_hour = st_day.slice(starttime=hour_start, endtime=hour_end)\n",
        "\n",
        "            # Skip if there's no data for the hour\n",
        "            if len(st_hour) == 0:\n",
        "                print(f\"Data gap detected for {station} on {day_start}. Skipping this hour.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                st_hour.detrend(\"spline\", order=5, dspline=1000)\n",
        "            except ValueError as e:\n",
        "                if \"Interior knots t must satisfy Schoenberg-Whitney conditions\" in str(e):\n",
        "                    print(f\"Skipping iteration due to spline error: {e}\")\n",
        "                    continue  # Skip this iteration and proceed with the next one\n",
        "                else:\n",
        "                    raise  # If it's a different ValueError, re-raise it\n",
        "\n",
        "            st_hour.filter(\"highpass\", freq=1.3, corners=3, zerophase=True)\n",
        "\n",
        "            while True:\n",
        "                n = len(st_hour.traces[0].data)\n",
        "                dt = st_hour.traces[0].stats.delta  # Sampling interval\n",
        "                freqs = np.fft.rfftfreq(n, dt)  # Frequency array\n",
        "                fft_vals = np.fft.rfft(st_hour.traces[0].data)  # Fourier transform\n",
        "                amplitude = np.abs(fft_vals)\n",
        "                max_index = np.argmax(amplitude[0:45000])\n",
        "                notch_freq = freqs[max_index]\n",
        "                quality_factor = 30  # Quality factor (higher value = narrower notch)\n",
        "                sampling_rate = st_hour[0].stats.sampling_rate  # Sampling rate (Hz)\n",
        "                b, a = iirnotch(w0=notch_freq, Q=quality_factor, fs=sampling_rate)\n",
        "\n",
        "                # Use a copy to apply the filter, then assign back\n",
        "                filtered_stream = st_hour.copy()\n",
        "                for i, trace in enumerate(filtered_stream):\n",
        "                    st_hour[i].data = filtfilt(b, a, trace.data)\n",
        "\n",
        "                if 1 == 1: # This condition is always true, so the loop runs only once.\n",
        "                    break\n",
        "\n",
        "            try:\n",
        "                st_hour.remove_response(inventory=inventory, pre_filt=pre_filt)\n",
        "            except ValueError as e:\n",
        "                print(f\"ValueError encountered while processing {station} at hour {hour} on {day_start}: {e}\")\n",
        "                continue  # Skip this hour if there's an error\n",
        "\n",
        "            # This is the line you mentioned\n",
        "            st_hour.detrend(\"spline\", order=3, dspline=700)\n",
        "\n",
        "            # --- MODIFIED SECTION START ---\n",
        "\n",
        "            output_dir = \"/content/Mseeds\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # 1. Define the output filename with a .mseed extension\n",
        "            filename = f\"{station}_day{day}_hour{hour}.mseed\"\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            # 2. Save the processed Stream object to a MiniSEED file\n",
        "            # The .write() method handles the conversion.\n",
        "            st_hour.write(output_path, format=\"MSEED\")\n",
        "\n",
        "            print(f\"Successfully saved data to {output_path}\")\n",
        "\n",
        "            # --- MODIFIED SECTION END ---"
      ],
      "metadata": {
        "id": "Kg0QhT5gcNiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from obspy import read\n",
        "\n",
        "# Define the path to the mseed file\n",
        "mseed_file_path = \"/content/Mseeds/AKOS_day0_hour0.mseed\"\n",
        "\n",
        "# Read the mseed file\n",
        "try:\n",
        "    st_mseed = read(mseed_file_path)\n",
        "\n",
        "    # Plot the data\n",
        "    print(f\"Plotting data from: {mseed_file_path}\")\n",
        "    st_mseed.plot()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{mseed_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading or plotting the file: {e}\")"
      ],
      "metadata": {
        "id": "zMH2GHvZcpch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import glob\n",
        "from obspy import read\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import re # Import the regular expression module for natural sorting\n",
        "\n",
        "# === STEP 1: DEFINE THE NATURAL SORTING FUNCTION ===\n",
        "# This function will be used as the key for sorting the file list.\n",
        "# It splits a filename into text and number parts, allowing for a natural sort.\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "# Define the directory containing the mseed files\n",
        "mseed_directory = \"/content/Mseeds\"\n",
        "\n",
        "# === STEP 2: FIND AND SORT FILES USING THE NEW KEY ===\n",
        "# Find all .mseed files in the directory\n",
        "all_files = glob.glob(os.path.join(mseed_directory, \"*.mseed\"))\n",
        "\n",
        "# Sort the files using our custom natural_sort_key\n",
        "mseed_files = sorted(all_files, key=natural_sort_key)\n",
        "\n",
        "# Check if any files were found\n",
        "if not mseed_files:\n",
        "    print(f\"No .mseed files found in {mseed_directory}\")\n",
        "else:\n",
        "    streams_to_plot = []\n",
        "    print(f\"Reading {len(mseed_files)} mseed files from {mseed_directory} in the following order:\")\n",
        "    # Optional: Print the sorted file order to verify\n",
        "    for f in mseed_files:\n",
        "        print(f\"  - {os.path.basename(f)}\")\n",
        "\n",
        "    for mseed_file in mseed_files:\n",
        "        try:\n",
        "            st = read(mseed_file)\n",
        "            streams_to_plot.append(st)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {mseed_file}: {e}\")\n",
        "\n",
        "    # === STEP 3: APPLY PLOT (NO CHANGES HERE) ===\n",
        "    if streams_to_plot:\n",
        "        print(\"\\nPlotting individual streams in one window...\")\n",
        "\n",
        "        # Create a figure and a set of subplots\n",
        "        fig, axes = plt.subplots(nrows=len(streams_to_plot), ncols=1, sharex=True, figsize=(12, 2 * len(streams_to_plot)))\n",
        "\n",
        "        # Handle the case of a single file, where 'axes' is not an array\n",
        "        if len(streams_to_plot) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        # Loop through each sorted stream and its corresponding axis\n",
        "        for st, ax in zip(streams_to_plot, axes):\n",
        "            # Iterate through each Trace in the Stream\n",
        "            for tr in st:\n",
        "                # Use Matplotlib's plot() directly\n",
        "                ax.plot(tr.times(\"matplotlib\"), tr.data, label=tr.id)\n",
        "\n",
        "            ax.set_ylabel(\"Amplitude\")\n",
        "            ax.legend(loc='upper right') # Add a legend to identify traces\n",
        "            ax.grid(True) # Add a grid for better readability\n",
        "\n",
        "        # Format the shared x-axis to display dates nicely\n",
        "        axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M:%S'))\n",
        "        fig.autofmt_xdate()\n",
        "\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No data to plot after attempting to read files.\")"
      ],
      "metadata": {
        "id": "qHKcMaO3e-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import glob\n",
        "from obspy import read, UTCDateTime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import re # For natural sorting\n",
        "\n",
        "# === 1. NATURAL SORTING FUNCTION ===\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Sorts strings with numbers in a human-friendly order (e.g., file2 before file10).\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "# === 2. SETUP AND FILE READING ===\n",
        "# Define the directory containing the mseed files\n",
        "mseed_directory = \"/content/Mseeds\"\n",
        "\n",
        "# Find and naturally sort all .mseed files\n",
        "all_files = glob.glob(os.path.join(mseed_directory, \"*.mseed\"))\n",
        "mseed_files = sorted(all_files, key=natural_sort_key)\n",
        "\n",
        "if not mseed_files:\n",
        "    print(f\"No .mseed files found in {mseed_directory}\")\n",
        "else:\n",
        "    stream_chunks = []\n",
        "    print(f\"Reading and splitting {len(mseed_files)} files into 1-hour segments...\")\n",
        "\n",
        "    for mseed_file in mseed_files:\n",
        "        try:\n",
        "            st = read(mseed_file)\n",
        "            if not st:\n",
        "                print(f\"  - Skipping empty file: {os.path.basename(mseed_file)}\")\n",
        "                continue\n",
        "\n",
        "            st.merge(method=1)\n",
        "\n",
        "            # === 3. SPLIT STREAM INTO 1-HOUR CHUNKS ===\n",
        "            stream_start = st[0].stats.starttime\n",
        "            stream_end = st[0].stats.endtime\n",
        "            current_time = stream_start\n",
        "\n",
        "            while current_time < stream_end:\n",
        "                chunk = st.slice(starttime=current_time, endtime=current_time + 3600) # 3600s = 1 hour\n",
        "                if chunk:\n",
        "                    stream_chunks.append(chunk)\n",
        "                current_time += 3600\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  - Error processing file {os.path.basename(mseed_file)}: {e}\")\n",
        "\n",
        "    # === 4. PLOT ALL THE 1-HOUR CHUNKS ===\n",
        "    if stream_chunks:\n",
        "        print(f\"\\nPlotting {len(stream_chunks)} one-hour segments in a single window...\")\n",
        "\n",
        "        # NOTE: sharex is intentionally False (default) so each plot has its own time window.\n",
        "        fig, axes = plt.subplots(\n",
        "            nrows=len(stream_chunks),\n",
        "            ncols=1,\n",
        "            figsize=(16, 2.5 * len(stream_chunks))\n",
        "        )\n",
        "\n",
        "        if len(stream_chunks) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for chunk, ax in zip(stream_chunks, axes):\n",
        "            start_time_str = chunk[0].stats.starttime.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Plot the 1-hour waveform\n",
        "            for tr in chunk:\n",
        "                ax.plot(tr.times(\"matplotlib\"), tr.data, label=tr.id)\n",
        "\n",
        "            ax.set_title(f\"1-Hour Waveform starting at: {start_time_str}\")\n",
        "            ax.set_ylabel(\"Amplitude\")\n",
        "            ax.legend(loc='upper right')\n",
        "            ax.grid(True)\n",
        "\n",
        "            # === FIX: APPLY TIME FORMATTER TO EVERY SUBPLOT ===\n",
        "            # This line is now INSIDE the loop.\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "\n",
        "        # Set the x-axis label only for the bottom plot\n",
        "        plt.xlabel(\"Time (H:M:S)\")\n",
        "        fig.tight_layout(pad=1.0)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nNo valid data segments found to plot.\")"
      ],
      "metadata": {
        "id": "pgmhhWDsgyWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##fetching"
      ],
      "metadata": {
        "id": "WZXZ8PAQBu4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the path to the .npy file\n",
        "npy_file_path = \"/content/AutoCorr_Outputs/AKOS_day0_hour0.npy\"\n",
        "\n",
        "# Read the .npy file\n",
        "try:\n",
        "    data = np.load(npy_file_path)\n",
        "\n",
        "    # Display the data (e.g., its shape and a few values)\n",
        "    print(f\"Successfully read data from: {npy_file_path}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    display(data)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{npy_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")"
      ],
      "metadata": {
        "id": "Tvw4BGRAvTy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbe03247"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# 1. Define paths\n",
        "source_directory = \"/content/AutoCorr_Outputs\"\n",
        "output_directory = \"/content/Final\"\n",
        "output_filename = \"combined_data.npy\"\n",
        "output_path = os.path.join(output_directory, output_filename)\n",
        "slice_length = 60000 # The first 60000 values to keep\n",
        "\n",
        "# 2. List and sort files\n",
        "# Using the natural sorting key defined previously (assuming it's available in the environment)\n",
        "# If not, redefine it here:\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Sorts strings with numbers in a human-friendly order (e.g., file2 before file10).\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "all_npy_files = glob.glob(os.path.join(source_directory, \"*.npy\"))\n",
        "sorted_npy_files = sorted(all_npy_files, key=natural_sort_key)\n",
        "\n",
        "if not sorted_npy_files:\n",
        "    print(f\"No .npy files found in {source_directory}\")\n",
        "else:\n",
        "    print(f\"Found {len(sorted_npy_files)} .npy files. Processing...\")\n",
        "    processed_data_list = []\n",
        "\n",
        "    # 3. Process and collect data\n",
        "    for file_path in sorted_npy_files:\n",
        "        try:\n",
        "            arr = np.load(file_path)\n",
        "            # Ensure the array has at least slice_length elements\n",
        "            if arr.size >= slice_length:\n",
        "                # Take the first slice_length elements. Assuming the data is 1D or can be flattened.\n",
        "                # If the arrays are consistently 2D with shape (1, N), arr[0, :slice_length] is appropriate.\n",
        "                # Based on the output of Tvw4BGRAvTy_, the shape is (1, 360001).\n",
        "                sliced_arr = arr[0, :slice_length]\n",
        "                processed_data_list.append(sliced_arr)\n",
        "                print(f\"Processed {os.path.basename(file_path)}. Sliced shape: {sliced_arr.shape}\")\n",
        "            else:\n",
        "                print(f\"Skipping {os.path.basename(file_path)}: array size ({arr.size}) is less than required ({slice_length}).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "    # 4. Combine data\n",
        "    if processed_data_list:\n",
        "        # Stack the sliced arrays vertically to create a single 2D array\n",
        "        combined_data = np.vstack(processed_data_list)\n",
        "        print(f\"\\nCombined data shape: {combined_data.shape}\")\n",
        "\n",
        "        # 5. Create output directory\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "        print(f\"Ensured output directory '{output_directory}' exists.\")\n",
        "\n",
        "        # 6. Save combined data\n",
        "        np.save(output_path, combined_data)\n",
        "        print(f\"Successfully saved combined data to '{output_path}'\")\n",
        "\n",
        "        # 7. Finish task - Implicitly done by the print statement above\n",
        "    else:\n",
        "        print(\"\\nNo data was processed to combine.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "aJ4oVVAqoPSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for custom accuracy\n",
        "def regression_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates a custom accuracy metric for regression tasks.\n",
        "    A prediction is considered \"accurate\" if its absolute error is\n",
        "    less than a certain threshold (e.g., 5% of the true value's range).\n",
        "    \"\"\"\n",
        "    # Define a tolerance. Let's say 5% of the typical signal range.\n",
        "    # IMPORTANT: You should adjust this threshold based on your data!\n",
        "    # If your normalized data is between 0 and 1, a 0.05 threshold is 5%.\n",
        "    TOLERANCE = 0.05\n",
        "\n",
        "    # Calculate the absolute difference between the true and predicted values\n",
        "    error = tf.abs(y_true - y_pred)\n",
        "\n",
        "    # Check where the error is less than our tolerance\n",
        "    is_accurate = tf.less(error, TOLERANCE)\n",
        "\n",
        "    # Cast boolean (True/False) to float (1.0/0.0) and find the mean\n",
        "    # This gives the percentage of \"accurate\" predictions\n",
        "    return tf.reduce_mean(tf.cast(is_accurate, tf.float32))"
      ],
      "metadata": {
        "id": "C0BRL-gOq_wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOLERANCE = 0.05 # Global tolerance for accuracy calculation\n",
        "\n",
        "def regression_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    A prediction is \"accurate\" if its absolute error is less than TOLERANCE.\n",
        "    \"\"\"\n",
        "    error = tf.abs(y_true - y_pred)\n",
        "    is_accurate = tf.less(error, TOLERANCE)\n",
        "    return tf.reduce_mean(tf.cast(is_accurate, tf.float32))"
      ],
      "metadata": {
        "id": "wgB0pUt9uVs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutocorrelationLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Computes the full autocorrelation of the input signal.\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AutocorrelationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.shape) == 3 and inputs.shape[-1] == 1:\n",
        "            inputs = tf.squeeze(inputs, axis=-1)\n",
        "        f = tf.signal.rfft(inputs)\n",
        "        power_spectral_density = f * tf.math.conj(f)\n",
        "        autocorr = tf.signal.irfft(power_spectral_density)\n",
        "        return autocorr"
      ],
      "metadata": {
        "id": "FLCoZLBduXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape=(60000,), final_output_points=1000):\n",
        "    \"\"\"Builds the model: Input -> Full Autocorrelation -> Dense -> Output\"\"\"\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    autocorr_output = AutocorrelationLayer(name='autocorrelation_layer')(inputs)\n",
        "    dense_output = tf.keras.layers.Dense(units=final_output_points, activation='relu')(autocorr_output)\n",
        "    final_output = tf.keras.layers.Reshape((1, final_output_points))(dense_output)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=final_output)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "pHcDF-fTuZ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_target_data(input_signals, num_points):\n",
        "    \"\"\"Calculates the autocorrelation for each signal and slices the first `num_points`.\"\"\"\n",
        "    signals_tensor = tf.convert_to_tensor(input_signals, dtype=tf.float32)\n",
        "    f = tf.signal.rfft(signals_tensor)\n",
        "    power_spectral_density = f * tf.math.conj(f)\n",
        "    autocorr = tf.signal.irfft(power_spectral_density)\n",
        "    target = autocorr[:, :num_points].numpy()\n",
        "    return target.reshape(target.shape[0], 1, target.shape[1])"
      ],
      "metadata": {
        "id": "faEgDU-4ucYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Final/combined_data.npy'\n",
        "output_dir = '/content/Final'\n",
        "FINAL_OUTPUT_POINTS = 1000\n",
        "\n",
        "try:\n",
        "    # --- Setup and Data Preparation ---\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found. Creating a dummy data file...\")\n",
        "        dummy_data = np.random.randn(48, 60000)\n",
        "        np.save(file_path, dummy_data)\n",
        "        print(\"Dummy file created.\")\n",
        "\n",
        "    X = np.load(file_path)\n",
        "    print(f\"Input data (X) loaded. Shape: {X.shape}\")\n",
        "\n",
        "    y = create_target_data(X, num_points=FINAL_OUTPUT_POINTS)\n",
        "    print(f\"Target data (y) created. Shape: {y.shape}\")\n",
        "\n",
        "    # --- Model Building and Compilation ---\n",
        "    model = build_model(\n",
        "        input_shape=(60000,),\n",
        "        final_output_points=FINAL_OUTPUT_POINTS\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mae', regression_accuracy]\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel Architecture Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # --- Model Training on the Entire Dataset ---\n",
        "    print(\"\\nStarting model training on the entire dataset...\")\n",
        "    history = model.fit(\n",
        "        X, y, # <-- Pass the entire dataset (X, y)\n",
        "        epochs=10,\n",
        "        batch_size=8,\n",
        "        shuffle=True # Good practice to shuffle data during training\n",
        "        # No validation_data argument is needed\n",
        "    )\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # --- Save the Final Trained Model in H5 Format ---\n",
        "    model_save_path = os.path.join(output_dir, 'final_model.h5')\n",
        "    model.save(model_save_path)\n",
        "    print(f\"\\n Final model saved successfully to: {model_save_path}\")\n",
        "\n",
        "    # --- Extract and Save Autocorrelation Layer Output ---\n",
        "    print(\"\\nExtracting output from the Autocorrelation layer...\")\n",
        "    autocorr_extractor_model = tf.keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.get_layer('autocorrelation_layer').output\n",
        "    )\n",
        "\n",
        "    # Use the first sample from the full dataset to get the result\n",
        "    sample_input = X[0:1]\n",
        "    autocorr_result = autocorr_extractor_model.predict(sample_input)\n",
        "\n",
        "    autocorr_save_path = os.path.join(output_dir, 'autocorrelation_result.npy')\n",
        "    np.save(autocorr_save_path, autocorr_result)\n",
        "    print(f\" Autocorrelation result for one sample saved successfully to: {autocorr_save_path}\")\n",
        "    print(f\"Shape of the saved autocorrelation result: {autocorr_result.shape}\")\n",
        "\n",
        "    # --- Visualization of Training Loss ---\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
        "    print(\"\\nChart of training history saved to 'training_history.png'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "BPgWUVY0ugt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define the path to the autocorrelation result file\n",
        "autocorr_file_path = \"/content/Final/autocorrelation_result.npy\"\n",
        "\n",
        "# Load the autocorrelation result\n",
        "try:\n",
        "    autocorr_data = np.load(autocorr_file_path)\n",
        "\n",
        "    print(f\"Successfully loaded autocorrelation data from: {autocorr_file_path}\")\n",
        "    print(f\"Data shape: {autocorr_data.shape}\")\n",
        "\n",
        "    # Assuming the data is a single sample (shape (1, N) or (N,))\n",
        "    # Flatten if it's (1, N) to get a 1D array for plotting\n",
        "    if autocorr_data.shape[0] == 1:\n",
        "        autocorr_data_1d = autocorr_data.flatten()\n",
        "    else:\n",
        "        autocorr_data_1d = autocorr_data # Assume it's already 1D or can be plotted as is\n",
        "\n",
        "    # Plot the autocorrelation result\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(autocorr_data_1d)\n",
        "    plt.title(\"Autocorrelation Result\")\n",
        "    plt.xlabel(\"Lag Index\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{autocorr_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading or plotting the file: {e}\")"
      ],
      "metadata": {
        "id": "7qzxjifNw4oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define the path to the autocorrelation result file\n",
        "autocorr_file_path = \"/content/Final/autocorrelation_result.npy\"\n",
        "\n",
        "# Load the autocorrelation result\n",
        "try:\n",
        "    autocorr_data = np.load(autocorr_file_path)\n",
        "\n",
        "    print(f\"Successfully loaded autocorrelation data from: {autocorr_file_path}\")\n",
        "    print(f\"Data shape: {autocorr_data.shape}\")\n",
        "\n",
        "    # Assuming the data is a single sample (shape (1, N) or (N,))\n",
        "    # Flatten if it's (1, N) to get a 1D array\n",
        "    if autocorr_data.shape[0] == 1:\n",
        "        autocorr_data_1d = autocorr_data.flatten()\n",
        "    else:\n",
        "        autocorr_data_1d = autocorr_data # Assume it's already 1D or can be plotted as is\n",
        "\n",
        "    # Slice the first 100 elements (from index 0 to 99)\n",
        "    sliced_data = autocorr_data_1d[:100]\n",
        "\n",
        "    # Plot the first 100 elements\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(sliced_data)\n",
        "    plt.title(\"First 100 Elements of Autocorrelation Result\")\n",
        "    plt.xlabel(\"Lag Index\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{autocorr_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading or plotting the file: {e}\")"
      ],
      "metadata": {
        "id": "izsdIody1esa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sampelling"
      ],
      "metadata": {
        "id": "Xt2LFzrMQuhI"
      }
    }
  ]
}